# ğŸ§  TextIntellect â€“ Document Q&A Bot (RAG + GenAI)
*A Retrieval-Augmented Generation (RAG) chatbot that answers questions from your uploaded documents.*

---

## ğŸ“Œ Table of Contents
- [Overview](#overview)
- [Features](#features)
- [How It Works (RAG Explained)](#how-it-works-rag-explained)
- [Project Architecture](#project-architecture)
- [Tech Stack](#tech-stack)
- [Installation](#installation)
- [Usage Guide](#usage-guide)
- [Configuration](#configuration)
- [Project Structure Details](#project-structure-details)
- [Example Code Flow](#example-code-flow)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [Contact](#contact)

---

# ğŸš€ Overview

**TextIntellect** is an AI-powered **Document Question-Answering Bot** built using **LangChain**, **HuggingFace Embeddings**, **ChromaDB**, and **Streamlit**.

Users can upload **PDF, DOCX, or TXT files**, and ask questions directly from the content.  
The chatbot retrieves relevant excerpts and uses a **Generative AI model** to provide context-aware answers.

This project demonstrates one of the **most in-demand AI engineering skills** today â€” building **RAG-based chatbots**.

---

# âœ¨ Features

- ğŸ“„ Upload documents (PDF, DOCX, TXT)
- ğŸ” Intelligent text extraction & chunking
- ğŸ§  HuggingFace semantic embeddings
- ğŸ—‚ï¸ ChromaDB vector storage
- ğŸ’¬ Contextual, document-grounded responses
- âš¡ Fast and scalable RAG pipeline
- ğŸ–¥ï¸ Modern Streamlit interface
- ğŸ“Œ Accurate, citation-based retrieval

---

# ğŸ§© How It Works (RAG Explained)

TextIntellect uses **Retrieval-Augmented Generation**, combining search + generation:

### **1ï¸âƒ£ Load**
Extract text from uploaded document.

### **2ï¸âƒ£ Chunk**
Split long text into small, meaningful pieces.

### **3ï¸âƒ£ Embed**
Convert each chunk into vectors using HuggingFace embeddings.

### **4ï¸âƒ£ Store**
Save embeddings in **ChromaDB** (vector database).

### **5ï¸âƒ£ Query**
Embed user question â†’ retrieve most relevant chunks.

### **6ï¸âƒ£ Generate**
Pass chunks as **context** to an LLM â†’ generate final answer.

---

# ğŸ—ï¸ Project Architecture
```
TextIntellect/
â”‚
â”œâ”€â”€ app.py # Streamlit UI
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # Documentation
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ settings.py # Configurations (chunk size, models, paths)
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ uploaded_files/ # Uploaded user documents
â”‚ â””â”€â”€ vector_store/ # ChromaDB persistent storage
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ loader.py # Document loading (PDF, DOCX, TXT)
â”‚ â”œâ”€â”€ chunker.py # Text chunking
â”‚ â”œâ”€â”€ embedder.py # Embedding generation
â”‚ â”œâ”€â”€ vectorstore.py # ChromaDB operations
â”‚ â”œâ”€â”€ llm.py # LLM integration
â”‚ â”œâ”€â”€ rag_pipeline.py # Complete RAG flow
â”‚ â””â”€â”€ utils.py # Helper utilities
â”‚
â””â”€â”€ tests/
â”œâ”€â”€ test_loader.py
â”œâ”€â”€ test_chunker.py
â””â”€â”€ test_rag.py
```

---

# ğŸ› ï¸ Tech Stack

**AI & NLP**
- LangChain
- HuggingFace Transformers
- Sentence-Transformers
- ChromaDB

**Frontend**
- Streamlit

**Backend**
- Python 3.10+

**Document Processing**
- PyPDF
- python-docx

---

# ğŸ“¦ Installation

## 1ï¸âƒ£ Clone Repository
```bash
git clone https://github.com/01mayankk/TextIntellect
cd TextIntellect
```
## 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```
## 3ï¸âƒ£ Run Application
```bash
streamlit run app.py
```

---

# ğŸ“ Usage Guide

### 1. Start the Streamlit App
The Streamlit UI will open automatically in your browser after running the app.

### 2. Upload a Document
Supported document types:
- **PDF**
- **DOCX**
- **TXT**

### 3. Ask a Question
Examples:
- â€œSummarize this document.â€
- â€œWhat are the key points?â€
- â€œWhat projects are mentioned in this resume?â€

### 4. View the Results
The app will display:
- **Final answer generated by the LLM**
- **Top retrieved relevant chunks from the document**

---
## âš™ï¸ Configuration

All configuration settings are located in `config/settings.py`.

```python
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
LLM_MODEL = "gpt-4o-mini"
VECTOR_STORE_PATH = "data/vector_store"
```
## ğŸ“ Project Structure Details

### **app.py**
- Handles the user interface  
- Provides file upload functionality  
- Connects Streamlit â†’ RAG pipeline  

### **loader.py**
- Reads input files (PDF, DOCX, TXT)  
- Extracts and cleans raw text  

### **chunker.py**
- Splits text into overlapping chunks  
- Maintains natural sentence boundaries for better context  

### **embedder.py**
- Loads HuggingFace embedding model  
- Converts processed text chunks into embeddings  

### **vectorstore.py**
- Creates and manages ChromaDB  
- Stores embeddings persistently  
- Performs similarity search during retrieval  

### **llm.py**
- Connects to the LLM backend (OpenAI / HuggingFace)  
- Generates the final response after retrieval  

### **rag_pipeline.py**

Coordinates the entire RAG pipeline:
load â†’ chunk â†’ embed â†’ store â†’ retrieve â†’ answer

---

## ğŸ§ª Example Code Flow

```python
# Load document
text = load_document(file_path)

# Chunk text
chunks = split_into_chunks(text, CHUNK_SIZE, CHUNK_OVERLAP)

# Create embeddings
embeddings = embed_text(chunks)

# Save to ChromaDB
store_embeddings(chunks, embeddings)

# Process question
results = retrieve_similar_chunks(question)

# Generate answer
answer = generate_answer(results, question)
```
## ğŸ”® Future Enhancements

- ğŸ” **Support for multiple documents**
- ğŸŒ **Deploy on HuggingFace Spaces / Render**
- ğŸ—ƒï¸ **Add FAISS as vector store option**
- ğŸ’¬ **Multi-turn conversation memory**
- ğŸ™ï¸ **Voice question answering**
- ğŸ“Š **UI improvements with citations**

---

## ğŸ¤ Contributing

Contributions are welcome!  
Follow these steps:

1. **Fork** the repository  
2. **Create** a feature branch  
3. **Commit** your changes  
4. **Create** a Pull Request  

---

## ğŸ“¬ Contact

**Mayank Kumar**  
ğŸ“§ Email: *02mayankk@gmail.com*  
ğŸ”— GitHub: *01mayankk*

---


